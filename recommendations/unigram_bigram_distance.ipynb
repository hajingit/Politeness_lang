{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrap-up for each request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    \n",
    "    stops = set(nltk.corpus.stopwords.words(\"english\") + list(string.punctuation) + [\"``\", \"''\"])\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text, debug=False):\n",
    "        return [i for i in nltk.word_tokenize(text.lower()) if i not in Request.stops]\n",
    "    \n",
    "    @staticmethod\n",
    "    def unigram(tokens):\n",
    "        return Counter(tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bigram(tokens):\n",
    "        while len(tokens) < 2:\n",
    "            tokens.append(\"\")\n",
    "        return Counter(((tokens[i], tokens[i+1]) for i in range(len(tokens)-1)))\n",
    "    \n",
    "    def __init__(self, rid, text, score, ngram_mode):\n",
    "        assert ngram_mode in {\"unigram\", \"bigram\"}, \"illegal ngram mode\"\n",
    "        self._rid = rid\n",
    "        self._text = text\n",
    "        self._toks = Request.tokenize(text)\n",
    "        self._score = score\n",
    "        if ngram_mode == \"unigram\":\n",
    "            self._ngram = Request.unigram(self._toks)\n",
    "        else:\n",
    "            self._ngram = Request.bigram(self._toks)\n",
    "            \n",
    "    def distance(self, right):\n",
    "        #assert isinstance(right, Request), f\"wrong operand for Request.distance({type(right)})\"\n",
    "        keys = set(self._ngram.keys())\n",
    "        keys.update(right._ngram.keys())\n",
    "        return reduce(lambda x,y : x+y, map(lambda k : (self._ngram.get(k, 0) - right._ngram.get(k, 0))**2, keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack = pd.read_csv(\"Stanford_politeness_corpus/stack-exchange.annotated.csv\", index_col=1)\n",
    "df_wiki = pd.read_csv(\"Stanford_politeness_corpus/wikipedia.annotated.csv\", index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_stack, df_wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10956\n",
      "Index(['Community', 'Request', 'Score1', 'Score2', 'Score3', 'Score4',\n",
      "       'Score5', 'TurkId1', 'TurkId2', 'TurkId3', 'TurkId4', 'TurkId5',\n",
      "       'Normalized Score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build requests and find closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_requests = [Request(k, v[\"Request\"], v[\"Normalized Score\"], \"unigram\") for k, v in df[[\"Request\", \"Normalized Score\"]].to_dict(\"index\").items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Be', 'more', 'specific', 'as', 'to', 'what', 'you', 'are', 'specifying', 'as', '``', 'the', 'claim', \"''\", 'so', 'that', 'I', 'may', 'find', 'relevant', 'information', 'to', 'refute', '?']\n"
     ]
    }
   ],
   "source": [
    "text = 'Be more specific as to ' \\\n",
    "       'what you are specifying as \"the claim\" ' \\\n",
    "       'so that I may find relevant information to refute?'\n",
    "print(nltk.word_tokenize(text))\n",
    "new_req = Request(rid=-1, text=text, score=0, ngram_mode=\"unigram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['specific',\n",
       " 'specifying',\n",
       " 'claim',\n",
       " 'may',\n",
       " 'find',\n",
       " 'relevant',\n",
       " 'information',\n",
       " 'refute']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Request.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = min(unigram_requests, key=new_req.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok. Thank you for clarifying. Could you be more specific as to what you are specifying as \"the claim\" so that I may find relevant information to refute?'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret._text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_requests = [Request(k, v[\"Request\"], v[\"Normalized Score\"], \"bigram\") for k, v in df[[\"Request\", \"Normalized Score\"]].to_dict(\"index\").items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_req = Request(rid=-1, text=\"Come on, I haven't unblocked many people before. Ok?\", score=0, ngram_mode=\"bigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = min(bigram_requests, key=new_req.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry... haven't unblocked many people before. Is it ok now?\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret._text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
