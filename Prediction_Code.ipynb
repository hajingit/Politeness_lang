{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline Code for Politeness Rating Prediction**\n",
    "\n",
    "\n",
    "The following code imports the libraries used for the prediction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import vaderSentiment\n",
    "import textstat\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#note: depending on how you installed (e.g., using source code download versus pip install), you may need to import like this:\n",
    "#from vaderSentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "import politeness.api_util\n",
    "from politeness.api_util import get_scores_strategies_token_indices\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global Variables declarations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables:\n",
    "#used in logistic regression\n",
    "L2_REGULARIZATION_STRENGTH = 0.9\n",
    "\n",
    "#headers and parameters for perspective api call\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "params = (\n",
    "    ('key', 'AIzaSyBaMPpybrBfyWF54hvkFK1QuEBPPKmQh8M'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below describes a function for reading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(file_name):\n",
    "    \"\"\"\n",
    "    Reads from the data file and returns data frame\n",
    "    :param file_name: reads the file name\n",
    "    :return: return a data frame read from file\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_name)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**\n",
    "\n",
    "The code below describes the function for encoding features:\n",
    "The following features are included-\n",
    "1. Sentiment Scores: pos, neg and neu\n",
    "easiness to read scales:\n",
    "2. Flesch reading,\n",
    "3. Dale_chall reading,\n",
    "4. Gunning_foc score,\n",
    "5. Smog_index and\n",
    "6. Text standard scores.\n",
    "all these scores are included in the entire feature set\n",
    "7. Perspective api scores (toxicity scores for the entire text)\n",
    "8. Politeness score\n",
    "9. Impolite-ness score\n",
    "10. Politeness strategies\n",
    "11. POS tags\n",
    "\n",
    "For using any feature, please comment out any particular feature's comment string using '#' token.\n",
    "These features formed creates a feature matrix to be used by the code for prediction in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_encoder(dataobjects):\n",
    "    \"\"\"\n",
    "    Features included in the code are:\n",
    "    1. sentiment scores: pos, neg and neu\n",
    "    easiness to read scales:\n",
    "        2. flesch reading,\n",
    "        3. dale_chall reading,\n",
    "        4. gunning_foc score,\n",
    "        5. smog_index and\n",
    "        6. text standard scores.\n",
    "        all these scores are included in the entire feature set\n",
    "    7. perspective api scores (toxicity scores for the entire text)\n",
    "    8. politeness score\n",
    "    9. impolite-ness score\n",
    "    10. politeness strategies\n",
    "    11. POS tags\n",
    "\n",
    "    :param dataobjects: reads the data objects (data frame) which incorporate the text\n",
    "    :return: a feature encoded matrix of numeric entities for the entire data set\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    feature_dict = {}\n",
    "    feature_set = {}\n",
    "\n",
    "    cnt=0\n",
    "    for line in dataobjects:\n",
    "        if cnt == 0:\n",
    "            cnt=1\n",
    "            continue\n",
    "        feature_dict[cnt]={}\n",
    "        text = line[2]\n",
    "        #sentiment scores: scores with pos, neg and neutral scores:\n",
    "        #feature_sentiment_comment_string = \"\"\"\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        vs = analyzer.polarity_scores(text)\n",
    "        feature_dict[cnt]['pos']=vs['pos']\n",
    "        feature_dict[cnt]['neg']=vs['neg']\n",
    "        feature_dict[cnt]['neu']=vs['neu']\n",
    "        feature_set['pos']=1\n",
    "        feature_set['neg']=1\n",
    "        feature_set['neu']=1\n",
    "        #\"\"\"\n",
    "        \n",
    "        #easiness to read scores: flesch reading:\n",
    "        feature_flesch_reading_comment_string = \"\"\"\n",
    "        sc = textstat.flesch_reading_ease(text)\n",
    "        feature_dict[cnt]['easiness']=sc\n",
    "        feature_set['easiness']=1\n",
    "        #\"\"\"\n",
    "        \n",
    "        #easiness to read scores: dale chall reading:\n",
    "        feature_dale_chall_comment_string = \"\"\"\n",
    "        sc = textstat.dale_chall_readability_score(text)\n",
    "        feature_dict[cnt]['easiness_dale']=sc\n",
    "        feature_set['easines_dale']=1\n",
    "        #\"\"\"\n",
    "        \n",
    "        #easiness to read scores: gunning fog reading:\n",
    "        feature_gunning_fog_comment_string = \"\"\"\n",
    "        sc = textstat.gunning_fog(text)\n",
    "        feature_dict[cnt]['easiness_fog']=sc\n",
    "        feature_set['easines_fog']=1\n",
    "        #\"\"\"\n",
    "        \n",
    "        #easiness to read scores: smog index reading:\n",
    "        feature_smog_index_comment_string = \"\"\"\n",
    "        sc = textstat.smog_index(text)\n",
    "        feature_dict[cnt]['easiness_smog']=sc\n",
    "        feature_set['easines_smog']=1\n",
    "        #\"\"\"\n",
    "        \n",
    "        #easiness to read scores: text standard reading:\n",
    "        feature_txt_standard_comment_string = \"\"\"\n",
    "        sc = textstat.text_standard(text, float_output=False)\n",
    "        feature_dict[cnt]['easiness_standard']=sc\n",
    "        feature_set['easines_standard']=1\n",
    "        #\"\"\"\n",
    "\n",
    "        #preprocessing text to make readable for perspective api scores:\n",
    "        feature_perspective_api_string = \"\"\"\n",
    "        stry = str(text)\n",
    "        sent = ''\n",
    "        for a in stry:\n",
    "            if a==' ' or (a<='Z' and a>='A') or (a<='z' and a>='a') or (a<='9' and a>='0') or a=='?' or a=='.':\n",
    "                sent +=a\n",
    "\n",
    "        #perspective api scores call:\n",
    "        data = '{comment: {text:\"'+sent+'\"}, languages: [\"en\"], requestedAttributes: {TOXICITY:{}} }'\n",
    "        response = requests.post('https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze', headers=headers, params=params, data=data)\n",
    "        j = json.loads(response.text)\n",
    "        feature_dict[cnt]['toxicity'] =0.0\n",
    "        try:\n",
    "            feature_dict[cnt]['toxicity'] = j['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "        except:\n",
    "            try:\n",
    "                feature_dict[cnt]['toxicity'] = j['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "            except:\n",
    "                try:\n",
    "                    feature_dict[cnt]['toxicity'] = j['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "                except:\n",
    "                    try:\n",
    "                        feature_dict[cnt]['toxicity'] = j['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "                    except:\n",
    "                        feature_dict[cnt]['toxicity'] =0.0\n",
    "        feature_dict[cnt]['toxicity'] =0.0\n",
    "        feature_set['toxicity']=1\n",
    "        #\"\"\"\n",
    "        \n",
    "        #politeness strategies and politeness scores features:\n",
    "        feature_politeness_score_comment_string = \"\"\"\n",
    "        sc = get_scores_strategies_token_indices(text)\n",
    "        feature_dict[cnt]['score_polite']=sc['score_polite']\n",
    "        feature_dict[cnt]['score_impolite'] = sc['score_impolite']\n",
    "        feature_set['score_polite']=1\n",
    "        feature_set['score_impolite']=1\n",
    "    \n",
    "        #print(feature_dict[cnt]['score_polite'])\n",
    "        for a in sc['strategies']:\n",
    "            feature_dict[cnt][a]=1\n",
    "            feature_set[a]=1\n",
    "        #\"\"\"\n",
    "        \n",
    "        #POS tags in the text:\n",
    "        feature_pos_comment_string = \"\"\"\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if (str(token.pos_) not in feature_set):\n",
    "                feature_set[str(token.pos_)]=1\n",
    "\n",
    "            if not (str(token.pos_) in feature_dict[cnt]):\n",
    "                feature_dict[cnt][str(token.pos_)]=1\n",
    "            else:\n",
    "                feature_dict[cnt][str(token.pos_)]+=1\n",
    "        #\"\"\"\n",
    "        cnt+=1\n",
    "\n",
    "    #creating a systematic feature matrix from feature set\n",
    "    feature_matrix = []\n",
    "    for i in range(1, cnt):\n",
    "        feature_list = []\n",
    "        for key in feature_set.keys():\n",
    "            if key in feature_dict[i]:\n",
    "                feature_list.append(feature_dict[i][key])\n",
    "            else:\n",
    "                feature_list.append(0.0)\n",
    "        feature_matrix.append(feature_list)\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Data**:\n",
    "\n",
    "The following code is used for reading any particular data file.\n",
    "The data file used for reading and extracting features is accessed via the \n",
    "'df' dataframe object. This object makes an assumption for using 3rd column (2nd index) for getting text records.\n",
    "\n",
    "Please change the name of the file with the correct path for using some other data objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling read data\n",
    "df = readData('Project_Politeness/Batch_Binary_Scores.csv')\n",
    "df_labels = readData('three_labels_data.csv')\n",
    "\n",
    "#list of lists of data frame objects, dataobjects: list of list of training modules, labelobjects: list of list of test modules\n",
    "dataobjects = df.values.tolist()\n",
    "labelobjects = df_labels.values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below calls the feature_encoder method for the training dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_matrix = feature_encoder(dataobjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier Creation:**\n",
    "\n",
    "The following code uses different type of classifiers. Each of the classifiers is instantiated in 'clf' variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **Random Forest Classifier** as a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **Logistic Regression** as a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.LogisticRegression(C=L2_REGULARIZATION_STRENGTH, penalty='l2', n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **Gaussian Naive Bayes** as a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB(priors=None, var_smoothing=1e-09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **Multinomial Naive Bayes** as a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **Support Vector Machines** as a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel = 'linear', C = 1, probability = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Class Labels**\n",
    "\n",
    "The code below is used for creating class labels with the feature matrix for training dataset\n",
    "\n",
    "**English Dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting class labels and appending to feature matrix (english)\n",
    "Y = []\n",
    "cnt = 0\n",
    "for line in dataobjects:\n",
    "    if cnt==0:\n",
    "        cnt=1\n",
    "        continue\n",
    "    feature_train_matrix[cnt-1].append(line[-2])\n",
    "    cnt+=1\n",
    "\n",
    "X = np.array(feature_train_matrix)\n",
    "Xtrain = np.array(X[:,:-1])\n",
    "cnt = 0\n",
    "for line in dataobjects:\n",
    "    if cnt==0:\n",
    "        cnt=1\n",
    "        continue\n",
    "    Y.append(X[cnt-1][-1])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the classifier on the dataset: **English Dataset**\n",
    "\n",
    "Creating test classes and predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model on the dataset\n",
    "clf.fit(Xtrain[:1000],Y[:1000])\n",
    "\n",
    "#Creating feature matrix for test set\n",
    "#feature_label_matrix = feature_encoder(labelobjects)\n",
    "#Xtester = np.array(feature_label_matrix)\n",
    "Xtest = Xtrain[1000:]\n",
    "\n",
    "#Predicting class labels for dataset: english\n",
    "YtestEn = clf.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used for creating class labels with the feature matrix for training dataset\n",
    "\n",
    "**Chinese Dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting class labels and appending to feature matrix (Chinese)\n",
    "Y = []\n",
    "cnt = 0\n",
    "for line in dataobjects:\n",
    "    if cnt==0:\n",
    "        cnt=1\n",
    "        continue\n",
    "    feature_train_matrix[cnt-1][-1] = line[-1]\n",
    "    cnt+=1\n",
    "\n",
    "X = np.array(feature_train_matrix)\n",
    "Xtrain = np.array(X[:,:-1])\n",
    "cnt = 0\n",
    "for line in dataobjects:\n",
    "    if cnt==0:\n",
    "        cnt=1\n",
    "        continue\n",
    "    Y.append(X[cnt-1][-1])\n",
    "    cnt+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the classifier on the dataset: **Chinese Dataset**\n",
    "\n",
    "Creating test classes and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model on the dataset\n",
    "clf.fit(Xtrain[:1000],Y[:1000])\n",
    "\n",
    "#Creating feature matrix for test set\n",
    "#feature_label_matrix = feature_encoder(labelobjects)\n",
    "#Xtester = np.array(feature_label_matrix)\n",
    "Xtest = Xtrain[1000:]\n",
    "\n",
    "#Predicting class labels for dataset: chinese\n",
    "YtestCh = log_reg.predict(Xtest[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting precision recall f_score and accuracy scores for the dataset: **English Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "precision: 0.7449664429530202  recall:  0.7449664429530202  f-score:  0.7449664429530202  accuracy:  0.7449664429530202\n"
     ]
    }
   ],
   "source": [
    "#Getting precision recall f_score and accuracy scores for the dataset\n",
    "scores = precision_recall_fscore_support(Y[1000:], YtestEn, average='micro')\n",
    "acc = accuracy_score(Y[1000:], YtestEn)\n",
    "print('English\\nprecision:',scores[0],' recall: ',scores[1],' f-score: ',scores[2],' accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting precision recall f_score and accuracy scores for the dataset: **Chinese Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese\n",
      "precision: 0.7449664429530202  recall:  0.7449664429530202  f-score:  0.7449664429530202  accuracy:  0.7449664429530202\n"
     ]
    }
   ],
   "source": [
    "#Getting precision recall f_score and accuracy scores for the dataset\n",
    "scores = precision_recall_fscore_support(Y[1000:], YtestCh, average='micro')\n",
    "acc = accuracy_score(Y[1000:], YtestCh)\n",
    "print('Chinese\\nprecision:',scores[0],' recall: ',scores[1],' f-score: ',scores[2],' accuracy: ',acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#To be used only with label data\n",
    "#Comparison model for english and chinese results:\n",
    "if len(YtestEn)!=len(YtestCh):\n",
    "    print('Not same length')\n",
    "for i in range(len(YtestEn)):\n",
    "    if YtestEn[i]!=YtestCh[i]:\n",
    "        print(1)\n",
    "    else:\n",
    "        print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
